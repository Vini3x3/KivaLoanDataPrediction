{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization and utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(y_test, y_pred):\n",
    "    \n",
    "    space_tab = 3\n",
    "    \n",
    "    if len(y_test) != len(y_pred):\n",
    "        return 'length of true labels and predicted labels are not equal.'\n",
    "    \n",
    "    report = []\n",
    "    \n",
    "    # labels\n",
    "    labels = set(y_test)\n",
    "    for label in labels:\n",
    "        test = [1 if each==label else 0 for each in y_test]\n",
    "        pred = [1 if each==label else 0 for each in y_pred]\n",
    "        report.append([\n",
    "            label, \n",
    "            round(r2_score(test,pred), 3), \n",
    "            round(mean_squared_error(test, pred),3), \n",
    "            round(explained_variance_score(test,pred),3), \n",
    "            y_test.count(label)\n",
    "        ])\n",
    "    \n",
    "    # macro\n",
    "    macro = [\n",
    "        'macro avg', \n",
    "        round(sum([row[1] for row in report]) / len(labels),3), \n",
    "        round(sum([row[2] for row in report]) / len(labels),3), \n",
    "        round(sum([row[3] for row in report]) / len(labels),3), \n",
    "        sum([row[4] for row in report])\n",
    "    ]           \n",
    "    \n",
    "    # micro    \n",
    "    diff = [1 if y_test[i]==y_pred[i] else 0 for i in range(len(y_test))]\n",
    "    same = [1] * len(y_test)\n",
    "    micro = [\n",
    "        'micro avg', \n",
    "        round(r2_score(diff,same),3), \n",
    "        round(mean_squared_error(diff,same), 3),\n",
    "        round(explained_variance_score(diff,same), 3),\n",
    "        len(y_test)\n",
    "    ]\n",
    "    \n",
    "    #formatting\n",
    "    space = ['    ', '    ', '    ', '    ', '    ']    \n",
    "    header = ['    ', 'r2_score', 'mean_squared_error', 'explained_variance_score', 'support']\n",
    "    \n",
    "    # add all the things    \n",
    "    report.insert(0,space)\n",
    "    report.insert(0,header)\n",
    "    report.append(space)\n",
    "    report.append(micro)\n",
    "    report.append(macro)        \n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    col = []\n",
    "    for i in range(len(report[0])):\n",
    "        col.append(max([len(str(row[i])) for row in report])+space_tab)\n",
    "    \n",
    "    for row in report:\n",
    "        for i in range(len(row)):\n",
    "            result += str(row[i]).rjust(col[i], ' ')\n",
    "        result += '\\n'\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomUniform(row):\n",
    "    prob = np.random.uniform(0, sum(row))\n",
    "    progress = 0\n",
    "    for i in range(len(row)):\n",
    "        if progress + row[i] > prob:\n",
    "#             print(i)\n",
    "            return int(i)\n",
    "#             break\n",
    "        progress += row[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_raw = pd.read_csv('kiva_loans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_cod = pd.read_csv('kiva_loans_dummied.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_std = pd.read_csv('kiva_loans_standardized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized and One-hot Encoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features = ['repayment_interval_irregular', 'repayment_interval_monthly', 'repayment_interval_weekly', 'repayment_interval_bullet']\n",
    "predict_features = list(loan_std.columns)\n",
    "for each in label_features:\n",
    "    predict_features.remove(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = loan_std[label_features]\n",
    "X_std = loan_std[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "X = X_std\n",
    "# y = y\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index].values.tolist(), y.iloc[test_index].values.tolist()\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    ALL_PRED_LABEL.extend(model.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "\n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "#     break\n",
    "\n",
    "# print(classification_report(ALL_TRUE_LABEL, ALL_PRED_LABEL))\n",
    "# print(regression_report(ALL_TRUE_LABEL, ALL_PRED_LABEL))\n",
    "# print(confusion_matrix(ALL_TRUE_LABEL, ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ['repayment_interval_irregular','repayment_interval_monthly','repayment_interval_weekly','repayment_interval_bullet']\n",
    "\n",
    "reg_label = [np.where(row == np.amax(row))[0] for row in ALL_PRED_LABEL]\n",
    "reg_label = [choices[each[0]] for each in reg_label]\n",
    "\n",
    "tru_label = [np.where(row == np.amax(row))[0] for row in ALL_TRUE_LABEL]\n",
    "tru_label = [choices[each[0]] for each in tru_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "   repayment_interval_bullet       0.72      0.60      0.65     70728\n",
      "repayment_interval_irregular       0.86      0.75      0.80    257158\n",
      "  repayment_interval_monthly       0.78      0.87      0.82    342717\n",
      "   repayment_interval_weekly       0.00      0.00      0.00       602\n",
      "\n",
      "                   micro avg       0.80      0.80      0.80    671205\n",
      "                   macro avg       0.59      0.56      0.57    671205\n",
      "                weighted avg       0.80      0.80      0.80    671205\n",
      "\n",
      "[[ 42714   4231  23783      0]\n",
      " [  1666 194122  61370      0]\n",
      " [ 15340  27960 299417      0]\n",
      " [     0     17    585      0]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(tru_label,reg_label))\n",
    "print(confusion_matrix(tru_label,reg_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_filter = confusion_matrix(tru_label,reg_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_label = [np.dot(bayes_filter, row) for row in ALL_PRED_LABEL]\n",
    "bayes_label = [randomUniform(row) for row in bayes_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_label[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671205\n"
     ]
    }
   ],
   "source": [
    "# bayes_labels = [choices[i] for i in bayes_label]\n",
    "bayes_labels = []\n",
    "step = 0\n",
    "for i in bayes_label:\n",
    "    if i == 0:\n",
    "        bayes_labels.append(choices[0])\n",
    "    elif i == 1:\n",
    "        bayes_labels.append(choices[1])\n",
    "    elif i == 2:\n",
    "        bayes_labels.append(choices[2])\n",
    "    else:\n",
    "        bayes_labels.append(choices[3])\n",
    "    step += 1\n",
    "bayes_label = bayes_labels\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "   repayment_interval_bullet       0.98      0.02      0.04     70728\n",
      "repayment_interval_irregular       0.73      0.42      0.53    257158\n",
      "  repayment_interval_monthly       0.64      0.78      0.71    342717\n",
      "   repayment_interval_weekly       0.00      0.16      0.00       602\n",
      "\n",
      "                   micro avg       0.56      0.56      0.56    671205\n",
      "                   macro avg       0.59      0.35      0.32    671205\n",
      "                weighted avg       0.71      0.56      0.57    671205\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1538,  12138,  48234,   8818],\n",
       "       [    12, 107768,  99600,  49778],\n",
       "       [    25,  27521, 268474,  46697],\n",
       "       [     0,     80,    424,     98]], dtype=int64)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(tru_label,bayes_labels))\n",
    "print(confusion_matrix(tru_label,bayes_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1389,   681,  3034,     0],\n",
       "       [  277, 16876,  9766,     0],\n",
       "       [ 2669,  2267, 30109,     0],\n",
       "       [    0,     2,    51,     0]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_TRUE_LABEL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.258955  ,  0.76438522, -0.00121742, -0.02210999])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_PRED_LABEL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.76541168e+02, 1.29596062e+04, 2.38735676e+03, 1.46668178e+00])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2 = np.dot(bayes_filter, ALL_PRED_LABEL[0])\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "prob = np.random.uniform(0, sum(temp2))\n",
    "progress = 0\n",
    "for i in range(len(temp2)):\n",
    "    if progress + temp2[i] > prob:\n",
    "        print(i)\n",
    "        break\n",
    "    progress += temp2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
