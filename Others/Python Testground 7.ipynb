{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the possibilities of using a filter method: first fiter out all the possibilies of the `weekly` labels, then identify the remaining types of repayment interval.  This method is proposed based on the observation of high accuracy in k-fold validation when there is no `weekly` data.  However, at the conclusion, it is at best that a single Decision Tree can outperforms all the other models and feature selection.  The raw standardized and dummified dataset is already the ceiling of prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization and utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import graphviz\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.read_csv('kiva_loans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_encoded = pd.read_csv('kiva_loans_dummied.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_std = pd.read_csv('kiva_loans_standardized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Find a classifier to identify whether the data is a repayment intervel of weekly or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectino I: Only Dummified and Standardized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(loan_std.columns)\n",
    "selected_features.remove('repayment_interval_irregular')\n",
    "selected_features.remove('repayment_interval_monthly')\n",
    "selected_features.remove('repayment_interval_weekly')\n",
    "selected_features.remove('repayment_interval_bullet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_weekly_std = loan_std['repayment_interval_weekly']\n",
    "X = loan_std[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.72      0.71      0.71       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.86      0.86      0.86    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670433    170]\n",
      " [   174    428]]\n"
     ]
    }
   ],
   "source": [
    "dtree = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    dtree.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.00      0.00      0.00       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.50      0.50      0.50    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670603      0]\n",
      " [   602      0]]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, max_depth=3)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    forest.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(forest.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.00      0.00      0.00       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.50      0.50      0.50    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670602      1]\n",
      " [   602      0]]\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(lg.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since KNN and SVM suffer from the curse of dimentionality, it is nearly impossible to test them for 270+ columns/features.  Therefore, we conclude that only Decision Tree has a good prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Test with LDA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "X_lda = lda.fit_transform(X, y_weekly_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = pd.DataFrame(X_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.36      0.33      0.35       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.68      0.67      0.67    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670254    349]\n",
      " [   402    200]]\n"
     ]
    }
   ],
   "source": [
    "dtree = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_lda):\n",
    "    X_train, X_test = X_lda.iloc[train_index], X_lda.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    dtree.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.11      0.00      0.00       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.56      0.50      0.50    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670595      8]\n",
      " [   601      1]]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, max_depth=3)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_lda):\n",
    "    X_train, X_test = X_lda.iloc[train_index], X_lda.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    forest.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(forest.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.00      0.00      0.00       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.50      0.50      0.50    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670603      0]\n",
      " [   602      0]]\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_lda):\n",
    "    X_train, X_test = X_lda.iloc[train_index], X_lda.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(lg.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    670603\n",
      "           1       0.00      0.00      0.00       602\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    671205\n",
      "   macro avg       0.50      0.50      0.50    671205\n",
      "weighted avg       1.00      1.00      1.00    671205\n",
      "\n",
      "[[670603      0]\n",
      " [   602      0]]\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(n_neighbors=100)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_lda):\n",
    "    X_train, X_test = X_lda.iloc[train_index], X_lda.iloc[test_index]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index], y_weekly_std.iloc[test_index]\n",
    "    knn.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(knn.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma='auto')\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_lda):\n",
    "    X_train, X_test = X_lda.iloc[train_index[0:2000]], X_lda.iloc[test_index[0:2000]]\n",
    "    y_train, y_test = y_weekly_std.iloc[train_index[0:200]], y_weekly_std.iloc[test_index[0:200]]\n",
    "    svm.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(svm.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree has the best predictions among all the models.  Only Decision Tree and Random Forest succeed to predict `weekly` data, while Decision Tree has better accuracy than Random Forest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compare the performance, it is better that Decision Tree first filter out the possible weekly cases without LDA feature extraction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Find a classifier to identify whether the data is a repayment intervel of bullet, monthly or irregular.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I: Only Dummified and Standardized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the hypothesis\n",
    "\n",
    "y_no_weekly = loan['repayment_interval']\n",
    "y_no_weekly = y_no_weekly.loc[y_no_weekly != 'weekly']\n",
    "\n",
    "X_no_weekly = loan_std.loc[loan_std['repayment_interval_weekly'] < 1]\n",
    "\n",
    "selected_features = list(X_no_weekly.columns)\n",
    "selected_features.remove('repayment_interval_irregular')\n",
    "selected_features.remove('repayment_interval_monthly')\n",
    "selected_features.remove('repayment_interval_weekly')\n",
    "selected_features.remove('repayment_interval_bullet')\n",
    "\n",
    "X_no_weekly = X_no_weekly[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.84      0.85      0.85     70728\n",
      "   irregular       0.89      0.91      0.90    257158\n",
      "     monthly       0.92      0.90      0.91    342717\n",
      "\n",
      "   micro avg       0.90      0.90      0.90    670603\n",
      "   macro avg       0.88      0.89      0.89    670603\n",
      "weighted avg       0.90      0.90      0.90    670603\n",
      "\n",
      "[[ 59990   2607   8131]\n",
      " [  2463 235058  19637]\n",
      " [  8798  25025 308894]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 1 minutes for each iteration\n",
    "\n",
    "dtree = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X_no_weekly):    \n",
    "    print('happy')\n",
    "    X_train, X_test = X_no_weekly.iloc[train_index], X_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    dtree.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       1.00      0.00      0.00     70728\n",
      "   irregular       0.87      0.60      0.71    257158\n",
      "     monthly       0.65      0.94      0.77    342717\n",
      "\n",
      "   micro avg       0.71      0.71      0.71    670603\n",
      "   macro avg       0.84      0.51      0.49    670603\n",
      "weighted avg       0.77      0.71      0.67    670603\n",
      "\n",
      "[[    99   2721  67908]\n",
      " [     0 153807 103351]\n",
      " [     0  20476 322241]]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, max_depth=3)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_no_weekly):        \n",
    "    X_train, X_test = X_no_weekly.iloc[train_index], X_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    forest.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(forest.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.75      0.70      0.72     70728\n",
      "   irregular       0.85      0.77      0.81    257158\n",
      "     monthly       0.80      0.87      0.83    342717\n",
      "\n",
      "   micro avg       0.81      0.81      0.81    670603\n",
      "   macro avg       0.80      0.78      0.79    670603\n",
      "weighted avg       0.82      0.81      0.81    670603\n",
      "\n",
      "[[ 49254   3847  17627]\n",
      " [  2685 197631  56842]\n",
      " [ 13405  30518 298794]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 1 minutes for each iteration\n",
    "\n",
    "lg = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_no_weekly):\n",
    "    \n",
    "    X_train, X_test = X_no_weekly.iloc[train_index], X_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(lg.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "\n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 270+ features / columns, only Logistic Regression, Random Forest and Decision Tree can be tested.  Decision Tree has the best prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: LDA extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "X_lda_no_weekly = lda.fit_transform(X_no_weekly, y_no_weekly)\n",
    "X_lda_no_weekly = pd.DataFrame(X_lda_no_weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.72      0.73      0.72     70728\n",
      "   irregular       0.80      0.82      0.81    257158\n",
      "     monthly       0.83      0.82      0.83    342717\n",
      "\n",
      "   micro avg       0.81      0.81      0.81    670603\n",
      "   macro avg       0.79      0.79      0.79    670603\n",
      "weighted avg       0.81      0.81      0.81    670603\n",
      "\n",
      "[[ 51516   4386  14826]\n",
      " [  4277 211623  41258]\n",
      " [ 15621  46981 280115]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 4 seconds for each iteration\n",
    "\n",
    "dtree = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_lda_no_weekly):    \n",
    "    \n",
    "    X_train, X_test = X_lda_no_weekly.iloc[train_index], X_lda_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    dtree.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.70      0.71      0.70     70728\n",
      "   irregular       0.85      0.79      0.82    257158\n",
      "     monthly       0.81      0.85      0.83    342717\n",
      "\n",
      "   micro avg       0.81      0.81      0.81    670603\n",
      "   macro avg       0.79      0.78      0.78    670603\n",
      "weighted avg       0.81      0.81      0.81    670603\n",
      "\n",
      "[[ 50245   3710  16773]\n",
      " [  2869 202248  52041]\n",
      " [ 18804  32203 291710]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 10 seconds for each iteration\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=20, max_depth=3)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_lda_no_weekly):    \n",
    "    \n",
    "    X_train, X_test = X_lda_no_weekly.iloc[train_index], X_lda_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    forest.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(forest.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.76      0.81      0.79     70728\n",
      "   irregular       0.85      0.84      0.85    257158\n",
      "     monthly       0.86      0.85      0.86    342717\n",
      "\n",
      "   micro avg       0.84      0.84      0.84    670603\n",
      "   macro avg       0.82      0.84      0.83    670603\n",
      "weighted avg       0.84      0.84      0.84    670603\n",
      "\n",
      "[[ 57608   2376  10744]\n",
      " [  3893 217200  36065]\n",
      " [ 14388  37343 290986]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 5 seconds for each iteration\n",
    "knn = KNN(n_neighbors=30)\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_lda_no_weekly):    \n",
    "    \n",
    "    X_train, X_test = X_lda_no_weekly.iloc[train_index], X_lda_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    knn.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(knn.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.66      0.31      0.43      2191\n",
      "   irregular       0.82      0.82      0.82      7860\n",
      "     monthly       0.76      0.85      0.80      9949\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     20000\n",
      "   macro avg       0.75      0.66      0.68     20000\n",
      "weighted avg       0.78      0.78      0.77     20000\n",
      "\n",
      "[[ 687  145 1359]\n",
      " [ 117 6452 1291]\n",
      " [ 230 1240 8479]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 5 seconds for each iteration\n",
    "svm = SVC(gamma='auto')\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_lda_no_weekly):    \n",
    "    \n",
    "    X_train, X_test = X_lda_no_weekly.iloc[train_index[0:20000]], X_lda_no_weekly.iloc[test_index[0:2000]]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index[0:20000]], y_no_weekly.iloc[test_index[0:2000]]\n",
    "    svm.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(svm.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.73      0.64      0.68     70728\n",
      "   irregular       0.86      0.76      0.81    257158\n",
      "     monthly       0.79      0.87      0.83    342717\n",
      "\n",
      "   micro avg       0.81      0.81      0.81    670603\n",
      "   macro avg       0.79      0.76      0.77    670603\n",
      "weighted avg       0.81      0.81      0.80    670603\n",
      "\n",
      "[[ 45173   4128  21427]\n",
      " [  1652 195967  59539]\n",
      " [ 14898  28226 299593]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 7 seconds for each iteration\n",
    "\n",
    "lg = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_lda_no_weekly):    \n",
    "    \n",
    "    X_train, X_test = X_lda_no_weekly.iloc[train_index], X_lda_no_weekly.iloc[test_index]\n",
    "    y_train, y_test = y_no_weekly.iloc[train_index], y_no_weekly.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(lg.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree performs the best.  Although SVM has its potential, it reaches its limit when the rate of increase of trainig data cannot catch up with the performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree, again, is chosen for the model of prediction.  More importantly, it is better not to have LDA feature extraction.  Other extraction methods may be used, for example, best subset or Lasso.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Combined Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step of the prediction.  The prediction is divided into two steps: \n",
    "\n",
    "1. A decision tree specialized in identifying `weekly` repayment interval; if not, proceed to step 2.  \n",
    "2. Another decision tree identify whether the loan application is `bullet`, `monthly` or `irregular`.  \n",
    "\n",
    "Before ending the development of prediction, same test is run for a single decision tree to compare the performance.  Decision is chosen because it has the best classification among all the models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare for dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = loan['repayment_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(loan_std.columns)\n",
    "selected_features.remove('repayment_interval_irregular')\n",
    "selected_features.remove('repayment_interval_monthly')\n",
    "selected_features.remove('repayment_interval_bullet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loan_std[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "Finish Test Iteration  0\n",
      "135\n",
      "Finish Test Iteration  1\n",
      "148\n",
      "Finish Test Iteration  2\n",
      "199\n",
      "Finish Test Iteration  3\n",
      "34\n",
      "Finish Test Iteration  4\n",
      "4\n",
      "Finish Test Iteration  5\n",
      "2\n",
      "Finish Test Iteration  6\n",
      "1\n",
      "Finish Test Iteration  7\n",
      "1\n",
      "Finish Test Iteration  8\n",
      "0\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.84      0.85      0.84     70728\n",
      "   irregular       0.89      0.91      0.90    257158\n",
      "     monthly       0.92      0.90      0.91    342717\n",
      "      weekly       0.71      0.70      0.70       602\n",
      "\n",
      "   micro avg       0.90      0.90      0.90    671205\n",
      "   macro avg       0.84      0.84      0.84    671205\n",
      "weighted avg       0.90      0.90      0.90    671205\n",
      "\n",
      "[[ 59976   2585   8156     11]\n",
      " [  2475 234928  19617    138]\n",
      " [  8811  24925 308953     28]\n",
      " [     5    147     26    424]]\n"
     ]
    }
   ],
   "source": [
    "# from experiment, 7 seconds for each iteration\n",
    "\n",
    "dtree1 = DTC()\n",
    "dtree2 = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):    \n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    selected_columns = list(X.columns)\n",
    "    selected_columns.remove('repayment_interval_weekly')\n",
    "    \n",
    "    # make a dataset only contains `weekly` and `non-weekly` label\n",
    "    dataset_weekly_train, dataset_weekly_test = X_train[selected_columns], X_test[selected_columns]\n",
    "    label_weekly_train, label_weekly_test = X_train['repayment_interval_weekly'], X_test['repayment_interval_weekly']\n",
    "    \n",
    "    # make a dataset without `weekly` data and label\n",
    "    dataset_no_weekly_train, dataset_no_weekly_test = X_train.loc[X_train['repayment_interval_weekly'] < 1], X_test.loc[X_test['repayment_interval_weekly'] < 1]\n",
    "    dataset_no_weekly_train, dataset_no_weekly_test = dataset_no_weekly_train[selected_columns], dataset_no_weekly_test[selected_columns]\n",
    "    label_no_weekly_train, label_no_weekly_test = y_train.loc[y_train != 'weekly'], y_test.loc[y_test != 'weekly']\n",
    "        \n",
    "    # train a dtree for recognizing `weekly` or not\n",
    "    dtree1.fit(dataset_weekly_train,label_weekly_train)\n",
    "        \n",
    "    # train a dtree for recognizing `irregular`,`monthly` and `bullet`        \n",
    "    dtree2.fit(dataset_no_weekly_train,label_no_weekly_train)\n",
    "    \n",
    "    # start prediction\n",
    "    y_pred_1 = dtree1.predict(dataset_weekly_test)  \n",
    "    y_pred_2 = dtree2.predict(dataset_weekly_test)\n",
    "    \n",
    "    print(sum(y_pred_1))\n",
    "    \n",
    "    # Merge Prediction Result\n",
    "    y_pred = []\n",
    "    for j in range(len(test_index)):\n",
    "        if y_pred_1[j]:\n",
    "            y_pred.append('weekly')\n",
    "        else:\n",
    "            y_pred.append(y_pred_2[j])\n",
    "\n",
    "    ALL_PRED_LABEL.extend(y_pred)\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "#     break\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Subset Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  1\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  2\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  3\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  4\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  5\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  6\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  7\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  8\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  9\n",
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "Finish Subset Iteration  10\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "j = 1\n",
    "for j in range(1,11):\n",
    "    \n",
    "    X_subset = SelectKBest(f_classif, k=j*27).fit_transform(X, y)\n",
    "    X_subset = pd.DataFrame(X_subset)\n",
    "    \n",
    "    dtree = DTC()\n",
    "\n",
    "    ALL_TRUE_LABEL = []\n",
    "    ALL_PRED_LABEL = []\n",
    "    kf = KFold(n_splits=10)\n",
    "    i = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X_subset):    \n",
    "\n",
    "        X_train, X_test = X_subset.iloc[train_index], X_subset.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        dtree.fit(X_train,y_train)\n",
    "        ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "        ALL_TRUE_LABEL.extend(y_test)\n",
    "\n",
    "        # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "        print('Finish Test Iteration ',i)\n",
    "        i += 1\n",
    "    score.append(precision_score(ALL_TRUE_LABEL, ALL_PRED_LABEL, average = 'macro'))\n",
    "    print('Finish Subset Iteration ',j)\n",
    "#     print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "#     print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 54, 81, 108, 135, 162, 189, 216, 243, 270]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_subset = [i * 27 for i in range(1,11)]\n",
    "k_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_subset, score)\n",
    "# plt.xlabel('Precision(macro)')\n",
    "# plt.ylabel('Number of Selected Columns')\n",
    "plt.title('Best Subset Selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it is better to use all features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Model Prediction (Decision Tree)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = loan['repayment_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(loan_std.columns)\n",
    "selected_features.remove('repayment_interval_weekly')\n",
    "selected_features.remove('repayment_interval_irregular')\n",
    "selected_features.remove('repayment_interval_monthly')\n",
    "selected_features.remove('repayment_interval_bullet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loan_std[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Test Iteration  0\n",
      "Finish Test Iteration  1\n",
      "Finish Test Iteration  2\n",
      "Finish Test Iteration  3\n",
      "Finish Test Iteration  4\n",
      "Finish Test Iteration  5\n",
      "Finish Test Iteration  6\n",
      "Finish Test Iteration  7\n",
      "Finish Test Iteration  8\n",
      "Finish Test Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bullet       0.84      0.85      0.85     70728\n",
      "   irregular       0.89      0.91      0.90    257158\n",
      "     monthly       0.92      0.90      0.91    342717\n",
      "      weekly       0.73      0.70      0.72       602\n",
      "\n",
      "   micro avg       0.90      0.90      0.90    671205\n",
      "   macro avg       0.85      0.84      0.84    671205\n",
      "weighted avg       0.90      0.90      0.90    671205\n",
      "\n",
      "[[ 59982   2620   8113     13]\n",
      " [  2450 234882  19712    114]\n",
      " [  8802  24944 308942     29]\n",
      " [     3    147     29    423]]\n"
     ]
    }
   ],
   "source": [
    "dtree = DTC()\n",
    "\n",
    "ALL_TRUE_LABEL = []\n",
    "ALL_PRED_LABEL = []\n",
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):    \n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    dtree.fit(X_train,y_train)\n",
    "    ALL_PRED_LABEL.extend(dtree.predict(X_test))\n",
    "    ALL_TRUE_LABEL.extend(y_test)\n",
    "    \n",
    "    # Screen Output for tracking the progress, sometimes I wait too long......\n",
    "    print('Finish Test Iteration ',i)\n",
    "    i += 1\n",
    "    \n",
    "print(classification_report(ALL_TRUE_LABEL,ALL_PRED_LABEL))\n",
    "print(confusion_matrix(ALL_TRUE_LABEL,ALL_PRED_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single model selection is even better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization is as important as accurate prediction.  Below we try to visualize the Decision Tree of single model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DTC()    \n",
    "dtree.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotfile = open(\"dtree.dot\", 'w')\n",
    "tree.export_graphviz(dtree, out_file = dotfile, feature_names = X.columns)\n",
    "dotfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the data are in normal distribution.  It is difficult to read, as data like `term_in_months` are transformed to float, e.g. 1.123.  In order to have better visualization, another decision tree is trained for this purpose.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(loan_encoded.columns)\n",
    "selected_features.remove('repayment_interval_weekly')\n",
    "selected_features.remove('repayment_interval_irregular')\n",
    "selected_features.remove('repayment_interval_monthly')\n",
    "selected_features.remove('repayment_interval_bullet')\n",
    "X_encoded = loan_encoded[selected_features]\n",
    "y = loan['repayment_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DTC()    \n",
    "dtree.fit(X_encoded,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotfile2 = open(\"dtree2.dot\", 'w')\n",
    "tree.export_graphviz(dtree, out_file = dotfile2, feature_names = X.columns)\n",
    "dotfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result is too large, only the first 1000 nodes are explored from 107922 nodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value in each nodes are refers to the outcomes / possible labels.  They are in order of bullet, irregular, monthly, weekly, same as the order in classification report.  As one traverses down the tree, fewer cases in the result arary are observed, which means the possibility narrows down.  The Left arrow is True, while Right arrow is False.  Each split is binary (either True or False).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see there are several improtant split: \n",
    "\n",
    "- terms in months < 2.5: (split A)\n",
    "\n",
    "No `monthly` outcome.  \n",
    "\n",
    "- country_code_KE < 0.5 (i.e. not KE): (if split A is True)\n",
    "\n",
    "No `weekly` outcome.  \n",
    "\n",
    "- count_female < 0.5 (i.e. no female borrower): (if split A is False) (split B)\n",
    "\n",
    "No `weekly` outcome.  \n",
    "\n",
    "- country_SV < 0.5 (i.e. not SV): (if split A is False and split B is False)\n",
    "\n",
    "No `monthly` outcome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few splits are related to `country_code`, `terms_in_months` and `sector_Agriculture`.  This shows that the location, the industry and the length of repayment much affects the repayment interval.  It is also clear that the data is highly specific, all feature extraction fails to give higher performance to the prediction.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
